# Bronze Operations SQL Files

This directory contains all SQL operations for the Bronze layer of the platform observability solution. These files are externalized from Python code for better maintainability, version control, and separation of concerns.

## üìÅ File Structure

| File | Purpose | Parameters | Description |
|------|---------|------------|-------------|
| `upsert_billing_usage.sql` | Upsert billing usage data | `{target_table}`, `{source_table}` | Merges billing usage records with change detection |
| `upsert_list_prices.sql` | Upsert list prices data | `{target_table}`, `{source_table}` | Merges pricing information for cost calculations |
| `upsert_job_run_timeline.sql` | Upsert job run timeline | `{target_table}`, `{source_table}` | Merges job execution timeline data |
| `upsert_job_task_run_timeline.sql` | Upsert job task timeline | `{target_table}`, `{source_table}` | Merges individual task execution data |
| `upsert_lakeflow_jobs.sql` | Upsert lakeflow jobs | `{target_table}`, `{source_table}` | Merges lakeflow job metadata |
| `upsert_lakeflow_pipelines.sql` | Upsert lakeflow pipelines | `{target_table}`, `{source_table}` | Merges lakeflow pipeline metadata |
| `upsert_compute_clusters.sql` | Upsert compute clusters | `{target_table}`, `{source_table}` | Merges cluster configuration and status |
| `upsert_compute_node_types.sql` | Upsert compute node types | `{target_table}`, `{source_table}` | Merges node type specifications |
| `upsert_access_workspaces.sql` | Upsert workspace access | `{target_table}`, `{source_table}` | Merges workspace metadata |

## üîß Usage

### **Parameter Substitution**

All SQL files use parameter placeholders that are replaced at runtime:

```sql
-- Example from upsert_billing_usage.sql
MERGE INTO {target_table} T
USING {source_table} S
ON T.workspace_id = S.workspace_id
-- ... rest of merge logic
```

### **Python Integration**

```python
from libs.sql_manager import sql_manager

# Load and parameterize SQL
sql = sql_manager.parameterize_sql(
    "upsert_billing_usage",
    target_table="platform_observability.plt_bronze.brz_billing_usage",
    source_table="stg_usage"
)

# Execute
spark.sql(sql)
```

## üìù SQL File Guidelines

### **1. Parameter Naming**
- Use descriptive parameter names: `{target_table}`, `{source_table}`
- Keep parameters simple and focused
- Document all parameters in the file header

### **2. SQL Structure**
- Use consistent formatting and indentation
- Include clear comments explaining complex logic
- Follow Databricks SQL best practices

### **3. Error Handling**
- Use appropriate MERGE logic for upsert operations
- Include change detection where applicable (row_hash comparisons)
- Handle NULL values appropriately

### **4. Performance**
- Optimize JOIN conditions for better performance
- Use appropriate indexes and partitioning
- Consider Z-ORDER optimization for common query patterns

## üß™ Testing

### **SQL Validation**
```python
from libs.sql_manager import sql_manager

# Validate SQL operations
assert sql_manager.validate_sql("upsert_billing_usage")
assert sql_manager.validate_sql("upsert_list_prices")
```

### **Parameter Testing**
```python
# Test parameter substitution
sql = sql_manager.parameterize_sql(
    "upsert_billing_usage",
    target_table="test.target",
    source_table="test.source"
)

# Verify parameters are substituted correctly
assert "test.target" in sql
assert "test.source" in sql
```

## üîÑ Maintenance

### **Adding New Operations**
1. Create new SQL file in this directory
2. Follow naming convention: `upsert_<entity_name>.sql`
3. Use consistent parameter naming
4. Add comprehensive comments
5. Update this README
6. Add tests to `test_sql_manager.py`

### **Modifying Existing Operations**
1. Update SQL file with new logic
2. Test parameter substitution still works
3. Verify Python integration
4. Update documentation if needed
5. Run tests to ensure compatibility

### **SQL File Caching**
The SQL manager caches loaded SQL files for performance. To reload:
```python
# Reload specific operation
sql_manager.reload_sql("upsert_billing_usage")

# Reload all operations
sql_manager.reload_sql()
```

## üìä Monitoring

### **SQL Execution Logging**
All SQL operations are logged with:
- Operation name
- Target and source tables
- SQL length for monitoring
- Execution success/failure status

### **Performance Metrics**
- SQL execution time
- Records processed
- Success/failure rates
- Parameter validation

## üö® Troubleshooting

### **Common Issues**

1. **File Not Found**
   - Verify SQL file exists in correct directory
   - Check file permissions
   - Ensure correct file naming

2. **Parameter Substitution Errors**
   - Verify parameter names match placeholders
   - Check for typos in parameter names
   - Ensure all required parameters are provided

3. **SQL Syntax Errors**
   - Validate SQL syntax in Databricks
   - Check for missing semicolons or brackets
   - Verify table and column names exist

### **Debug Mode**
Enable debug logging to see SQL content:
```python
logger.setLevel(logging.DEBUG)
```

## üìö Additional Resources

- [Databricks SQL Documentation](https://docs.databricks.com/sql/index.html)
- [Delta Lake MERGE Operations](https://docs.databricks.com/delta/delta-update.html)
- [Performance Tuning Guide](sql/performance_optimizations.sql)
- [Platform Observability Architecture](README.md)
