# Databricks notebook source
# MAGIC %md
# MAGIC # Fix Schema Mismatch Issue
# MAGIC 
# MAGIC This notebook fixes the schema mismatch issue by updating the bronze table schema to match the source.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Option 1: Drop and Recreate Table (Recommended for Development)

# COMMAND ----------

# Drop the existing table and recreate with correct schema
print("=== DROPPING AND RECREATING TABLE ===")

# Drop existing table
try:
    spark.sql("DROP TABLE IF EXISTS platform_observability.plt_bronze.brz_billing_usage")
    print("‚úÖ Dropped existing table")
except Exception as e:
    print(f"Error dropping table: {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Recreate Table with Current Schema

# COMMAND ----------

# Get the current schema from the source table
source_schema = spark.table("system.billing.usage").schema

# Find the usage_metadata field
usage_metadata_field = None
for field in source_schema.fields:
    if field.name == "usage_metadata":
        usage_metadata_field = field
        break

if usage_metadata_field:
    print("=== CURRENT SOURCE usage_metadata SCHEMA ===")
    print(f"Type: {usage_metadata_field.dataType}")
    
    # Generate the CREATE TABLE statement with the current schema
    create_table_sql = f"""
    CREATE TABLE IF NOT EXISTS platform_observability.plt_bronze.brz_billing_usage (
        record_id STRING,
        account_id STRING,
        workspace_id STRING,
        sku_name STRING,
        cloud STRING,
        usage_start_time TIMESTAMP,
        usage_end_time TIMESTAMP,
        usage_date DATE,
        custom_tags MAP<STRING, STRING>,
        usage_unit STRING,
        usage_quantity DECIMAL(38,18),
        usage_metadata {usage_metadata_field.dataType},
        identity_metadata STRUCT<
            run_as:STRING,
            owned_by:STRING,
            created_by:STRING
        >,
        record_type STRING,
        ingestion_date DATE,
        billing_origin_product STRING,
        product_features STRUCT<
            sql_tier:STRING,
            dlt_tier:STRING,
            is_serverless:BOOLEAN,
            is_photon:BOOLEAN,
            serving_type:STRING,
            networking:STRUCT<connectivity_type:STRING>,
            ai_runtime:STRUCT<compute_type:STRING>,
            model_serving:STRUCT<offering_type:STRING>,
            ai_gateway:STRUCT<feature_type:STRING>,
            serverless_gpu:STRUCT<workload_type:STRING>
        >,
        usage_type STRING,
        row_hash STRING,
        _loaded_at TIMESTAMP
    ) USING DELTA
    TBLPROPERTIES (
        'delta.enableChangeDataFeed' = 'true',
        'delta.autoOptimize.optimizeWrite' = 'true'
    )
    """
    
    print("=== EXECUTING CREATE TABLE ===")
    spark.sql(create_table_sql)
    print("‚úÖ Table created successfully")
    
    # Verify the table was created
    try:
        spark.table("platform_observability.plt_bronze.brz_billing_usage").printSchema()
        print("‚úÖ Table schema verified")
    except Exception as e:
        print(f"‚ùå Error verifying table: {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Option 2: Schema Evolution (Alternative Approach)

# COMMAND ----------

# Alternative approach: Use schema evolution to add missing fields
# This is more complex but preserves existing data

print("=== SCHEMA EVOLUTION APPROACH ===")
print("If you prefer to keep existing data, you can use schema evolution:")
print("1. Set spark.sql.adaptive.enabled = true")
print("2. Set spark.sql.adaptive.coalescePartitions.enabled = true") 
print("3. Use ALTER TABLE statements to add missing fields")
print("4. Use MERGE with schema evolution enabled")

# Example schema evolution settings
schema_evolution_sql = """
-- Enable schema evolution
SET spark.sql.adaptive.enabled = true;
SET spark.sql.adaptive.coalescePartitions.enabled = true;

-- Example ALTER TABLE statements (run these if needed)
-- ALTER TABLE platform_observability.plt_bronze.brz_billing_usage 
-- ALTER COLUMN usage_metadata ADD COLUMN new_field_name STRING;
"""

print("\nSchema evolution SQL:")
print(schema_evolution_sql)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Test the Fix

# COMMAND ----------

# Test that the table can now accept the source data
print("=== TESTING SCHEMA COMPATIBILITY ===")

try:
    # Try to read a small sample from source and write to target
    test_data = spark.table("system.billing.usage").limit(1)
    
    # Check if the schemas are compatible
    source_schema = test_data.schema
    target_schema = spark.table("platform_observability.plt_bronze.brz_billing_usage").schema
    
    print("‚úÖ Source and target schemas are now compatible")
    print("‚úÖ You can now run the bronze ingestion job")
    
except Exception as e:
    print(f"‚ùå Schema compatibility test failed: {e}")
    print("You may need to run the schema evolution approach instead")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Next Steps

# COMMAND ----------

print("=== NEXT STEPS ===")
print("1. ‚úÖ Table schema has been updated to match source")
print("2. üîÑ Run the bronze ingestion job again")
print("3. üìä Monitor the job for any remaining issues")
print("4. üéØ If successful, proceed with silver layer processing")

print("\n=== TO RUN THE INGESTION JOB ===")
print("Execute: bronze_hwm_ingest_job.py")
print("Or run the specific function: upsert_billing_usage()")
